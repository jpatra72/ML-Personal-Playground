{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM Playground.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXDAdKrw7v5aOs+gTMU/Dy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpatra72/ML-Personal-Playground/blob/main/LSTM_Playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Source Links to Playground Activity\n",
        "*   *Outputs of LSTM: https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm*\n",
        "\n",
        "*   *How to update LSTM during training: https://machinelearningmastery.com/update-lstm-networks-training-time-series-forecasting*\n",
        "\n",
        "*   *How to build LSTM (tf): https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767#.ozeai0fo8*\n",
        "\n",
        "* *BPTT in LSTM: https://stats.stackexchange.com/questions/219914/rnns-when-to-apply-bptt-and-or-update-weights*\n",
        "\n",
        "* *https://machinelearningmastery.com/prepare-univariate-time-series-data-long-short-term-memory-networks/*\n",
        "\n",
        "* *Keras LSTM diagram to understand Batch: https://github.com/MohammadFneish7/Keras_LSTM_Diagram*\n",
        "\n",
        "* *Stateless vs Statefull and Subsequencing: http://philipperemy.github.io/keras-stateful-lstm/*\n",
        "\n",
        "* *Pytorch vs TensorFlow - Stateless vs Statefull: https://discuss.pytorch.org/t/confusion-regarding-pytorch-lstms-compared-to-keras-stateful-lstm/44502/5*\n",
        "\n",
        "* *Stateful w/ Subsequencing:  https://gist.github.com/spacegoing/7935e5c2f0c8fa2f0719d2e729e794e8#file-test_stateful_lstm-py-L22*\n",
        "\n",
        "* *Pytorch forward Implementation: https://towardsdatascience.com/whats-happening-in-my-lstm-layer-dd8110ecc52f*\n",
        "\n",
        "* *Packing and Padding Sequences: https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec*\n",
        "\n",
        "* *Packing structure with figures: https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch*\n",
        "\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "# Secondary Topics\n",
        "* *Dataloader & Dataset: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html*\n",
        "\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "N5qnc3C-uxe5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIUw_zQYW-Mc"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "\n",
        "torch.manual_seed(1)\n",
        "inputs = [torch.randn(1, 3) for _ in range(5)] # indicates that there are 5 sequences to be given as inputs and (1,3) indicates that there is 1 layer with 3 cells\n",
        "hidden = (torch.randn(1, 1, 3),\n",
        "          torch.randn(1, 1, 3)) # initializing h and c values to be of dimensions (1, 1, 3) which \n",
        "                                #indicates there is (1 * 1) - num_layers * num_directions, with batch size of 1 and projection size of 3. \n",
        "                                # Since there is only 1 batch in input, h and c can also have only one batch of data for initialization \n",
        "                                #and the number of cells in both input and output should also match.\n",
        " \n",
        "lstm = nn.LSTM(3, 3) #implying both input and output are 3 dimensional data\n",
        "# summary(lstm, input_size=(1,3))\n",
        "for i in inputs:\n",
        "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "    print('out:', out)\n",
        "    print('hidden:', hidden, '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 2\n",
        "inputs = [torch.randn(1, 3) for _ in range(5)] \n",
        "hidden = (torch.randn(2, 1, 3),\n",
        "          torch.randn(2, 1, 3))\n",
        "lstm = nn.LSTM(input_size=3, hidden_size=3, num_layers=2)\n",
        "for i in inputs:\n",
        "    # Step through the sequence one element at a time.\n",
        "    # after each step, hidden contains the hidden state.\n",
        "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "    print('out:', out)\n",
        "    print('hidden:', hidden, '\\n')"
      ],
      "metadata": {
        "id": "ZwwoSjG7pihY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "lstm = nn.LSTM( input_size = 1, hidden_size = 20, num_layers  = 1 )\n",
        "x = torch.rand( 50, 1, 1)\n",
        "output, (hn, cn) = lstm(x)\n",
        "output.size()"
      ],
      "metadata": {
        "id": "MzwgQk5xuCdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
        "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
        "# print(inputs)\n",
        "# initialize the hidden state.\n",
        "hidden = (torch.randn(1, 1, 3),\n",
        "          torch.randn(1, 1, 3))\n",
        "for i in inputs:\n",
        "    # Step through the sequence one element at a time.\n",
        "    # after each step, hidden contains the hidden state.\n",
        "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "\n",
        "# alternatively, we can do the entire sequence all at once.\n",
        "# the first value returned by LSTM is all of the hidden states throughout\n",
        "# the sequence. the second is just the most recent hidden state\n",
        "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
        "# The reason for this is that:\n",
        "# \"out\" will give you access to all hidden states in the sequence\n",
        "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
        "# by passing it as an argument  to the lstm at a later time\n",
        "# Add the extra 2nd dimension\n",
        "# print(len(inputs))\n",
        "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
        "# print((inputs), '\\n')\n",
        "\n",
        "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
        "out, hidden = lstm(inputs, hidden)\n",
        "print(out)\n",
        "print((hidden))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIR2NdGPMalE",
        "outputId": "18bfc639-f74a-42df-a147-e32852851148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.3653,  0.0123, -0.4226]],\n",
            "\n",
            "        [[ 0.1415,  0.1479, -0.2528]],\n",
            "\n",
            "        [[ 0.4234,  0.0467, -0.1540]],\n",
            "\n",
            "        [[ 0.5676, -0.1238,  0.0710]],\n",
            "\n",
            "        [[ 0.7421, -0.0026,  0.2334]]], grad_fn=<StackBackward0>)\n",
            "(tensor([[[ 0.7421, -0.0026,  0.2334]]], grad_fn=<StackBackward0>), tensor([[[ 1.3536, -0.0061,  0.3241]]], grad_fn=<StackBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "import numpy as np\n",
        "from numpy.random import choice\n",
        "\n",
        "\n",
        "def prepare_sequences(x_train, window_length):\n",
        "  windows = []\n",
        "  for i, sequence in enumerate(x_train):\n",
        "    for window_start in range(0, T - window_length + 1):\n",
        "      window_end = window_start + window_length\n",
        "      window = sequence[window_start:window_end]\n",
        "      windows.append(window)\n",
        "  return np.array(windows)\n",
        "\n",
        "\n",
        "def get_sequential_batch(bX_train, bY_train, N_train, batch_size):\n",
        "  bX_train = bX_train.reshape(N_train, T - window_length + 1, window_length)\n",
        "  N = N_train - N_train % batch_size\n",
        "  for i in range(0, N, batch_size):\n",
        "    for t in range(T - window_length + 1):\n",
        "      bX = bX_train[i:i + batch_size, t, :]\n",
        "      bY = bY_train[i:i + batch_size]\n",
        "      yield bX[..., np.newaxis], bY[..., np.newaxis], t\n",
        "      # yield bX, bY, t\n",
        "\n",
        "\n",
        "## hyper parameters\n",
        "debug = True\n",
        "N = 1200\n",
        "T = 20\n",
        "N_train = 1000\n",
        "N_test = N - N_train\n",
        "window_length = 10\n",
        "batch_size = 32\n",
        "epochs = 4\n",
        "# if stateful = True, test acc = 1.0; False, test acc = 0.5\n",
        "stateful = False\n",
        "\n",
        "## create train / test dataset\n",
        "data = np.zeros([N, T])\n",
        "one_indexes = choice(a=N, size=N // 2, replace=False)\n",
        "data[one_indexes, 0] = 1  # very long term memory.\n",
        "X_train = data[:N_train]\n",
        "Y_train = X_train[:, 0]\n",
        "X_test = data[N_train:]\n",
        "Y_test = X_test[:, 0]\n",
        "\n",
        "## create model\n",
        "model = Sequential()\n",
        "model.add(\n",
        "    LSTM(\n",
        "        3,\n",
        "        batch_input_shape=(batch_size, window_length, 1),\n",
        "        return_sequences=False,\n",
        "        stateful=stateful))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(\n",
        "    loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "## training loop\n",
        "for e in range(epochs):\n",
        "  # train data generator\n",
        "  bX_train = prepare_sequences(X_train, window_length)\n",
        "  # print(bX_train.reshape(N_train, T - window_length + 1, window_length).shape)\n",
        "  x_train_batch_gen = get_sequential_batch(bX_train, Y_train, N_train,\n",
        "                                           batch_size)\n",
        "  for bX, bY, t in x_train_batch_gen:\n",
        "    print(bX.shape, t)\n",
        "    loss, acc = model.train_on_batch(bX, bY)\n",
        "    tr_loss.append(loss)\n",
        "    tr_acc.append(acc)\n",
        "    counter += 1\n",
        "\n",
        "    if counter == 1 and debug:\n",
        "      t_dataset.append(\n",
        "          sum(bY[:, 0] == bX[:, 0, :].reshape(-1)) + int(bX.sum() == bY.sum()))\n",
        "\n",
        "    # reset states\n",
        "    if counter == T - window_length + 1:\n",
        "      model.reset_states()\n",
        "      counter = 0\n",
        "  print(np.mean(tr_acc))\n",
        "  # debug\n",
        "  if debug:\n",
        "    print(np.mean(t_dataset))\n",
        "\n"
      ],
      "metadata": {
        "id": "DGXJsrfN0PEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class SimpleLSTM(nn.Module):\n",
        "  \"\"\"implements a 'simple' lstm - a single/multilayer uni/bi directional lstm with a single output\"\"\"\n",
        "  def __init__(self, n_features, window_size, \n",
        "               output_size, h_size, n_layers=1, \n",
        "               bidirectional=False, device=torch.device('cpu')):\n",
        "    super().__init__()\n",
        "    self.n_features = n_features\n",
        "    self.window_size = window_size\n",
        "    self.output_size = output_size\n",
        "    self.h_size = h_size\n",
        "    self.n_layers = n_layers\n",
        "    self.directions = 2 if bidirectional else 1\n",
        "    self.device = device\n",
        "\n",
        "    # our layer of interest\n",
        "    self.lstm = nn.LSTM(input_size=n_features, hidden_size=h_size, \n",
        "                        num_layers=n_layers, bidirectional=bidirectional, batch_first=True)\n",
        "    self.hidden = None\n",
        "    \n",
        "    self.linear = nn.Linear(self.h_size * self.directions, self.output_size)\n",
        "    \n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    \n",
        "    hidden_state  = torch.randn(self.n_layers * self.directions,\n",
        "                            batch_size ,self.h_size).to(self.device)\n",
        "    cell_state  = torch.randn(self.n_layers * self.directions, \n",
        "                           batch_size,self.h_size).to(self.device)\n",
        "    \n",
        "    hidden_state = Variable(hidden_state)\n",
        "    cell_state = Variable(cell_state)\n",
        "\n",
        "    return (hidden_state, cell_state) \n",
        "\n",
        "  def forward(self, input):\n",
        "    batch_size = list(input.size())[0]\n",
        "    self.hidden = self.init_hidden(batch_size)\n",
        "    lstm_output, self.hidden = self.lstm(input, self.hidden)\n",
        "    print(\"lstm_output:\", lstm_output.shape)\n",
        "    print(\"hidden:\", len(self.hidden), self.hidden[0].shape)\n",
        "    last_hidden_states = torch.index_select(lstm_output, 1,  index=torch.LongTensor(([self.window_size-1])))\n",
        "    predictions = self.linear(last_hidden_states)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "model = SimpleLSTM(n_features=23, window_size=6, output_size=1, h_size=256)\n",
        "\n",
        "data = torch.rand((100,6, 23))\n",
        "\n",
        "print(model.forward(data).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FvwU8S80U-C",
        "outputId": "0f358989-505b-47df-d4c6-1d1954c3aa7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object get_sequential_batch at 0x7f906977bed0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# Custom Dataset\n",
        "class TensorDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, TensorX,TensorY):\n",
        "        self.TensorX = TensorX\n",
        "        self.TensorY = TensorY\n",
        "    def __len__(self):\n",
        "        return self.TensorX.shape[0]\n",
        "    def __getitem__(self,idx):\n",
        "        return (self.TensorX[idx],self.TensorY[idx])\n",
        "\n",
        "# Model = Stateful LSTM+linear\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size,hidden_size,output_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(batch_first=True,input_size=input_size,hidden_size=hidden_size)\n",
        "        self.linear = torch.nn.Linear(in_features=hidden_size, out_features=output_size)\n",
        "    def forward(self, x, hn, cn):\n",
        "        # Stateful\n",
        "        x_longer = x.view(1,x.shape[0]*x.shape[1],x.shape[2])\n",
        "        out_longer, (hn, cn) = self.lstm(x_longer, (hn.detach(), cn.detach()))\n",
        "        out = out_longer.view(x.shape[0],x.shape[1],out_longer.shape[2])\n",
        "        print(\"output pre linear layer: \", out, out.shape)\n",
        "        print(out[:,-1,:], out[:,-1,:].shape)\n",
        "        out = self.linear(out[:,-1,:])\n",
        "        print(\"output post linear layer: \", out, out.shape)\n",
        "        return out.unsqueeze(-1), (hn, cn)\n",
        "\n",
        "N_epochs = 1\n",
        "hidden_size = 2\n",
        "features = 1\n",
        "learning_rate = 0.001\n",
        "batch_size=2\n",
        "output_size = 1\n",
        "model = LSTM(input_size=features,hidden_size=hidden_size,output_size=output_size)#Create model\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)#optimizer\n",
        "criterion = torch.nn.MSELoss() # loss\n",
        "# Create dataset: Imagine original_batch_size=2\n",
        "x = torch.tensor([[1.0, 2.0, 3.0],[4.0, 5.0, 6.0],[7.0, 8.0, 9.0],[10.0, 11.0, 12.0]]).unsqueeze(-1)\n",
        "y = torch.tensor([[4.],[7.],[10.],[13.]]).unsqueeze(-1)\n",
        "dataset = TensorDataset(x,y)\n",
        "dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size)\n",
        "print(\"head from dataloader: \", next(iter(dataloader))[1].shape)\n",
        "# Training\n",
        "for epoch in range(0,N_epochs):\n",
        "    # Create first hidden and cell state with batch=1 \n",
        "    hn = torch.zeros(1, 1, hidden_size)#[num_layers*num_directions,batch,hidden_size]\n",
        "    cn = torch.zeros(1, 1, hidden_size)#[num_layers*num_directions,batch,hidden_size]\n",
        "    for x,y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        out, (hn,cn) = model(x,hn,cn)\n",
        "        loss = criterion(out,y)\n",
        "        loss.backward()# Backward\n",
        "        optimizer.step()# gradient descent on adam step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9q0utQkXCyZ4",
        "outputId": "57ae3f6b-cb97-46ac-b0bd-91574f975347"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "head from dataloader:  torch.Size([2, 1, 1])\n",
            "output pre linear layer:  tensor([[[-0.1933, -0.0227],\n",
            "         [-0.4396,  0.1259],\n",
            "         [-0.5998,  0.4198]],\n",
            "\n",
            "        [[-0.6966,  0.6801],\n",
            "         [-0.7582,  0.7975],\n",
            "         [-0.7991,  0.8407]]], grad_fn=<ViewBackward0>) torch.Size([2, 3, 2])\n",
            "tensor([[-0.5998,  0.4198],\n",
            "        [-0.7991,  0.8407]], grad_fn=<SliceBackward0>) torch.Size([2, 2])\n",
            "output post linear layer:  tensor([[0.5430],\n",
            "        [0.6204]], grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
            "output pre linear layer:  tensor([[[-0.8269,  0.8639],\n",
            "         [-0.8490,  0.8801],\n",
            "         [-0.8667,  0.8940]],\n",
            "\n",
            "        [[-0.8813,  0.9063],\n",
            "         [-0.8936,  0.9172],\n",
            "         [-0.9041,  0.9268]]], grad_fn=<ViewBackward0>) torch.Size([2, 3, 2])\n",
            "tensor([[-0.8667,  0.8940],\n",
            "        [-0.9041,  0.9268]], grad_fn=<SliceBackward0>) torch.Size([2, 2])\n",
            "output post linear layer:  tensor([[0.6257],\n",
            "        [0.6281]], grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import LongTensor\n",
        "from torch.nn import Embedding, LSTM\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "## We want to run LSTM on a batch of 3 character sequences ['long_str', 'tiny', 'medium']\n",
        "#\n",
        "#     Step 1: Construct Vocabulary\n",
        "#     Step 2: Load indexed data (list of instances, where each instance is list of character indices)\n",
        "#     Step 3: Make Model\n",
        "#  *  Step 4: Pad instances with 0s till max length sequence\n",
        "#  *  Step 5: Sort instances by sequence length in descending order\n",
        "#  *  Step 6: Embed the instances\n",
        "#  *  Step 7: Call pack_padded_sequence with embeded instances and sequence lengths\n",
        "#  *  Step 8: Forward with LSTM\n",
        "#  *  Step 9: Call unpack_padded_sequences if required / or just pick last hidden vector\n",
        "#  *  Summary of Shape Transformations\n",
        "\n",
        "# We want to run LSTM on a batch following 3 character sequences\n",
        "seqs = ['long_str',  # len = 8\n",
        "        'tiny',      # len = 4\n",
        "        'medium']    # len = 6\n",
        "\n",
        "\n",
        "## Step 1: Construct Vocabulary ##\n",
        "##------------------------------##\n",
        "# make sure <pad> idx is 0\n",
        "vocab = ['<pad>'] + sorted(set([char for seq in seqs for char in seq]))\n",
        "# => ['<pad>', '_', 'd', 'e', 'g', 'i', 'l', 'm', 'n', 'o', 'r', 's', 't', 'u', 'y']\n",
        "\n",
        "\n",
        "## Step 2: Load indexed data (list of instances, where each instance is list of character indices) ##\n",
        "##-------------------------------------------------------------------------------------------------##\n",
        "vectorized_seqs = [[vocab.index(tok) for tok in seq]for seq in seqs]\n",
        "# vectorized_seqs => [[6, 9, 8, 4, 1, 11, 12, 10],\n",
        "#                     [12, 5, 8, 14],\n",
        "#                     [7, 3, 2, 5, 13, 7]]\n",
        "\n",
        "\n",
        "## Step 3: Make Model ##\n",
        "##--------------------##\n",
        "embed = Embedding(len(vocab), 4) # embedding_dim = 4\n",
        "lstm = LSTM(input_size=4, hidden_size=5, batch_first=True) # input_dim = 4, hidden_dim = 5\n",
        "\n",
        "\n",
        "## Step 4: Pad instances with 0s till max length sequence ##\n",
        "##--------------------------------------------------------##\n",
        "\n",
        "# get the length of each seq in your batch\n",
        "seq_lengths = LongTensor(list(map(len, vectorized_seqs)))\n",
        "# seq_lengths => [ 8, 4,  6]\n",
        "# batch_sum_seq_len: 8 + 4 + 6 = 18\n",
        "# max_seq_len: 8\n",
        "\n",
        "seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
        "# seq_tensor => [[0 0 0 0 0 0 0 0]\n",
        "#                [0 0 0 0 0 0 0 0]\n",
        "#                [0 0 0 0 0 0 0 0]]\n",
        "\n",
        "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "    seq_tensor[idx, :seqlen] = LongTensor(seq)\n",
        "# seq_tensor => [[ 6  9  8  4  1 11 12 10]          # long_str\n",
        "#                [12  5  8 14  0  0  0  0]          # tiny\n",
        "#                [ 7  3  2  5 13  7  0  0]]         # medium\n",
        "# seq_tensor.shape : (batch_size X max_seq_len) = (3 X 8)\n",
        "\n",
        "\n",
        "## Step 5: Sort instances by sequence length in descending order ##\n",
        "##---------------------------------------------------------------##\n",
        "\n",
        "seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
        "seq_tensor = seq_tensor[perm_idx]\n",
        "# seq_tensor => [[ 6  9  8  4  1 11 12 10]           # long_str\n",
        "#                [ 7  3  2  5 13  7  0  0]           # medium\n",
        "#                [12  5  8 14  0  0  0  0]]          # tiny\n",
        "# seq_tensor.shape : (batch_size X max_seq_len) = (3 X 8)\n",
        "\n",
        "\n",
        "## Step 6: Embed the instances ##\n",
        "##-----------------------------##\n",
        "\n",
        "embedded_seq_tensor = embed(seq_tensor)\n",
        "# embedded_seq_tensor =>\n",
        "#                       [[[-0.77578706 -1.8080667  -1.1168439   1.1059115 ]     l\n",
        "#                         [-0.23622951  2.0361056   0.15435742 -0.04513785]     o\n",
        "#                         [-0.6000342   1.1732816   0.19938554 -1.5976517 ]     n\n",
        "#                         [ 0.40524676  0.98665565 -0.08621677 -1.1728264 ]     g\n",
        "#                         [-1.6334635  -0.6100042   1.7509955  -1.931793  ]     _\n",
        "#                         [-0.6470658  -0.6266589  -1.7463604   1.2675372 ]     s\n",
        "#                         [ 0.64004815  0.45813003  0.3476034  -0.03451729]     t\n",
        "#                         [-0.22739866 -0.45782727 -0.6643252   0.25129375]]    r\n",
        "\n",
        "#                        [[ 0.16031227 -0.08209462 -0.16297023  0.48121014]     m\n",
        "#                         [-0.7303265  -0.857339    0.58913064 -1.1068314 ]     e\n",
        "#                         [ 0.48159844 -1.4886451   0.92639893  0.76906884]     d\n",
        "#                         [ 0.27616557 -1.224429   -1.342848   -0.7495876 ]     i\n",
        "#                         [ 0.01795524 -0.59048957 -0.53800726 -0.6611691 ]     u\n",
        "#                         [ 0.16031227 -0.08209462 -0.16297023  0.48121014]     m\n",
        "#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]     <pad>\n",
        "#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]]    <pad>\n",
        "\n",
        "#                        [[ 0.64004815  0.45813003  0.3476034  -0.03451729]     t\n",
        "#                         [ 0.27616557 -1.224429   -1.342848   -0.7495876 ]     i\n",
        "#                         [-0.6000342   1.1732816   0.19938554 -1.5976517 ]     n\n",
        "#                         [-1.284392    0.68294704  1.4064184  -0.42879772]     y\n",
        "#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]     <pad>\n",
        "#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]     <pad>\n",
        "#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]     <pad>\n",
        "#                         [ 0.2691206  -0.43435425  0.87935454 -2.2269666 ]]]   <pad>\n",
        "# embedded_seq_tensor.shape : (batch_size X max_seq_len X embedding_dim) = (3 X 8 X 4)\n",
        "\n",
        "\n",
        "## Step 7: Call pack_padded_sequence with embeded instances and sequence lengths ##\n",
        "##-------------------------------------------------------------------------------##\n",
        "\n",
        "packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\n",
        "# packed_input (PackedSequence is NamedTuple with 2 attributes: data and batch_sizes\n",
        "#\n",
        "# packed_input.data =>\n",
        "#                         [[-0.77578706 -1.8080667  -1.1168439   1.1059115 ]     l\n",
        "#                          [ 0.01795524 -0.59048957 -0.53800726 -0.6611691 ]     m\n",
        "#                          [-0.6470658  -0.6266589  -1.7463604   1.2675372 ]     t\n",
        "#                          [ 0.16031227 -0.08209462 -0.16297023  0.48121014]     o\n",
        "#                          [ 0.40524676  0.98665565 -0.08621677 -1.1728264 ]     e\n",
        "#                          [-1.284392    0.68294704  1.4064184  -0.42879772]     i\n",
        "#                          [ 0.64004815  0.45813003  0.3476034  -0.03451729]     n\n",
        "#                          [ 0.27616557 -1.224429   -1.342848   -0.7495876 ]     d\n",
        "#                          [ 0.64004815  0.45813003  0.3476034  -0.03451729]     n\n",
        "#                          [-0.23622951  2.0361056   0.15435742 -0.04513785]     g\n",
        "#                          [ 0.16031227 -0.08209462 -0.16297023  0.48121014]     i\n",
        "#                          [-0.22739866 -0.45782727 -0.6643252   0.25129375]]    y\n",
        "#                          [-0.7303265  -0.857339    0.58913064 -1.1068314 ]     _\n",
        "#                          [-1.6334635  -0.6100042   1.7509955  -1.931793  ]     u\n",
        "#                          [ 0.27616557 -1.224429   -1.342848   -0.7495876 ]     s\n",
        "#                          [-0.6000342   1.1732816   0.19938554 -1.5976517 ]     m\n",
        "#                          [-0.6000342   1.1732816   0.19938554 -1.5976517 ]     t\n",
        "#                          [ 0.48159844 -1.4886451   0.92639893  0.76906884]     r\n",
        "# packed_input.data.shape : (batch_sum_seq_len X embedding_dim) = (18 X 4)\n",
        "#\n",
        "# packed_input.batch_sizes => [ 3,  3,  3,  3,  2,  2,  1,  1]\n",
        "# visualization :\n",
        "# l  o  n  g  _  s  t  r   #(long_str)\n",
        "# m  e  d  i  u  m         #(medium)\n",
        "# t  i  n  y               #(tiny)\n",
        "# 3  3  3  3  2  2  1  1   (sum = 18 [batch_sum_seq_len])\n",
        "\n",
        "\n",
        "## Step 8: Forward with LSTM ##\n",
        "##---------------------------##\n",
        "\n",
        "packed_output, (ht, ct) = lstm(packed_input)\n",
        "# packed_output (PackedSequence is NamedTuple with 2 attributes: data and batch_sizes\n",
        "#\n",
        "# packed_output.data :\n",
        "#                          [[-0.00947162  0.07743231  0.20343193  0.29611713  0.07992904]   l\n",
        "#                           [ 0.08596145  0.09205993  0.20892891  0.21788561  0.00624391]   o\n",
        "#                           [ 0.16861682  0.07807446  0.18812777 -0.01148055 -0.01091915]   n\n",
        "#                           [ 0.20994528  0.17932937  0.17748171  0.05025435  0.15717036]   g\n",
        "#                           [ 0.01364102  0.11060348  0.14704391  0.24145307  0.12879576]   _\n",
        "#                           [ 0.02610307  0.00965587  0.31438383  0.246354    0.08276576]   s\n",
        "#                           [ 0.09527554  0.14521319  0.1923058  -0.05925677  0.18633027]   t\n",
        "#                           [ 0.09872741  0.13324396  0.19446367  0.4307988  -0.05149471]   r\n",
        "#                           [ 0.03895474  0.08449443  0.18839942  0.02205326  0.23149511]   m\n",
        "#                           [ 0.14620507  0.07822411  0.2849248  -0.22616537  0.15480657]   e\n",
        "#                           [ 0.00884941  0.05762182  0.30557525  0.373712    0.08834908]   d\n",
        "#                           [ 0.12460691  0.21189159  0.04823487  0.06384943  0.28563985]   i\n",
        "#                           [ 0.01368293  0.15872964  0.03759198 -0.13403234  0.23890573]   u\n",
        "#                           [ 0.00377969  0.05943518  0.2961751   0.35107893  0.15148178]   m\n",
        "#                           [ 0.00737647  0.17101538  0.28344846  0.18878219  0.20339936]   t\n",
        "#                           [ 0.0864429   0.11173367  0.3158251   0.37537992  0.11876849]   i\n",
        "#                           [ 0.17885767  0.12713005  0.28287745  0.05562563  0.10871304]   n\n",
        "#                           [ 0.09486895  0.12772645  0.34048414  0.25930756  0.12044918]]  y\n",
        "# packed_output.data.shape : (batch_sum_seq_len X hidden_dim) = (18 X 5)\n",
        "\n",
        "# packed_output.batch_sizes => [ 3,  3,  3,  3,  2,  2,  1,  1] (same as packed_input.batch_sizes)\n",
        "# visualization :\n",
        "# l  o  n  g  _  s  t  r   #(long_str)\n",
        "# m  e  d  i  u  m         #(medium)\n",
        "# t  i  n  y               #(tiny)\n",
        "# 3  3  3  3  2  2  1  1   (sum = 18 [batch_sum_seq_len])\n",
        "\n",
        "\n",
        "## Step 9: Call unpack_padded_sequences if required / or just pick last hidden vector ##\n",
        "##------------------------------------------------------------------------------------##\n",
        "\n",
        "# unpack your output if required\n",
        "output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
        "# output:\n",
        "# output =>\n",
        "#                          [[[-0.00947162  0.07743231  0.20343193  0.29611713  0.07992904]   l\n",
        "#                            [ 0.20994528  0.17932937  0.17748171  0.05025435  0.15717036]   o\n",
        "#                            [ 0.09527554  0.14521319  0.1923058  -0.05925677  0.18633027]   n\n",
        "#                            [ 0.14620507  0.07822411  0.2849248  -0.22616537  0.15480657]   g\n",
        "#                            [ 0.01368293  0.15872964  0.03759198 -0.13403234  0.23890573]   _\n",
        "#                            [ 0.00737647  0.17101538  0.28344846  0.18878219  0.20339936]   s\n",
        "#                            [ 0.17885767  0.12713005  0.28287745  0.05562563  0.10871304]   t\n",
        "#                            [ 0.09486895  0.12772645  0.34048414  0.25930756  0.12044918]]  r\n",
        "\n",
        "#                           [[ 0.08596145  0.09205993  0.20892891  0.21788561  0.00624391]   m\n",
        "#                            [ 0.01364102  0.11060348  0.14704391  0.24145307  0.12879576]   e\n",
        "#                            [ 0.09872741  0.13324396  0.19446367  0.4307988  -0.05149471]   d\n",
        "#                            [ 0.00884941  0.05762182  0.30557525  0.373712    0.08834908]   i\n",
        "#                            [ 0.00377969  0.05943518  0.2961751   0.35107893  0.15148178]   u\n",
        "#                            [ 0.0864429   0.11173367  0.3158251   0.37537992  0.11876849]   m\n",
        "#                            [ 0.          0.          0.          0.          0.        ]   <pad>\n",
        "#                            [ 0.          0.          0.          0.          0.        ]]  <pad>\n",
        "\n",
        "#                           [[ 0.16861682  0.07807446  0.18812777 -0.01148055 -0.01091915]   t\n",
        "#                            [ 0.02610307  0.00965587  0.31438383  0.246354    0.08276576]   i\n",
        "#                            [ 0.03895474  0.08449443  0.18839942  0.02205326  0.23149511]   n\n",
        "#                            [ 0.12460691  0.21189159  0.04823487  0.06384943  0.28563985]   y\n",
        "#                            [ 0.          0.          0.          0.          0.        ]   <pad>\n",
        "#                            [ 0.          0.          0.          0.          0.        ]   <pad>\n",
        "#                            [ 0.          0.          0.          0.          0.        ]   <pad>\n",
        "#                            [ 0.          0.          0.          0.          0.        ]]] <pad>\n",
        "# output.shape : ( batch_size X max_seq_len X hidden_dim) = (3 X 8 X 5)\n",
        "\n",
        "# Or if you just want the final hidden state?\n",
        "print(ht[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9dM8KTILlzT",
        "outputId": "d4cd8429-3a91-4974-808e-3e1800d0b2a0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1186,  0.4580,  0.2507, -0.0030, -0.1805],\n",
            "        [ 0.0495,  0.3380,  0.3448, -0.2172, -0.1773],\n",
            "        [ 0.0102,  0.3309,  0.0513, -0.2876, -0.1029]],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ti6EQUmFLrWg",
        "outputId": "e9d25f02-0566-42b5-a82b-746a491ecc97"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.2056,  1.0667,  0.4801, -0.0047, -0.3705],\n",
              "         [ 0.0628,  1.1215,  0.8265, -0.3935, -0.3716],\n",
              "         [ 0.0207,  0.5816,  0.0797, -0.4208, -0.2107]]],\n",
              "       grad_fn=<StackBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}